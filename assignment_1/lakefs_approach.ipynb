{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "version_1_path = local_path = os.path.expanduser('~/Documents/UChicago_MADS/general_datasets/athletes.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# lakeFS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run 'python -m lakefs.quickstart' in terminal to start lakeFS locally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "ENDPOINT = \"http://127.0.0.1:8000/\"\n",
    "ACCESS_KEY = \"AKIAIOSFOLQUICKSTART\"\n",
    "SECRET_KEY = \"wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a branch called version 1, add csv to that branch, commit the change"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Repository: lakefs://athletes\n",
      "\u001b[91merror creating repository: not unique\u001b[0m\n",
      "409 Conflict\n"
     ]
    }
   ],
   "source": [
    "! lakectl repo create lakefs://athletes local://example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source ref: lakefs://athletes/main\n",
      "\u001b[91mbranch already exists: not unique\u001b[0m\n",
      "409 Conflict\n"
     ]
    }
   ],
   "source": [
    "! lakectl branch create lakefs://athletes/assignment_1 --source lakefs://athletes/main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Path: \u001b[93mathletes.csv\u001b[0m\n",
      "Modified Time: 2025-10-15 19:56:48 -0400 EDT\n",
      "Size: 71546909 bytes\n",
      "Human Size: 71.5 MB\n",
      "Physical Address: local:///Users/jackploshnick/lakefs/data/block/example/data/g5fd5iiclaanklhe8940/d3o39g2claanklhe898g\n",
      "Checksum: ade8057a9ad4350dfade9180f021a96d\n",
      "Content-Type: application/octet-stream\n"
     ]
    }
   ],
   "source": [
    "! lakectl fs upload -s /Users/jackploshnick/Documents/UChicago_MADS/general_datasets/athletes.csv lakefs://athletes/assignment_1/athletes.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Branch: lakefs://athletes/assignment_1\n",
      "Commit for branch \"assignment_1\" completed.\n",
      "\n",
      "ID: \u001b[93m5d983aea16e5b39e4852e5f700083217eb4f18c6557ac1f18be7101ee71ce849\u001b[0m\n",
      "Message: Add version_1 dataset\n",
      "Timestamp: 2025-10-15 19:56:49 -0400 EDT\n",
      "Parents: a931d09e4064907b899d9d60be97dae2817c000ec5726bd14aa3471903c5d40c\n",
      "\n"
     ]
    }
   ],
   "source": [
    "! lakectl commit lakefs://athletes/assignment_1 -m \"Add version_1 dataset\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## update the dataet, make a new commit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(version_1_path)\n",
    "\n",
    "# Remove not relevant columns\n",
    "data = data.dropna(subset=['region','age','weight','height','howlong','gender','eat', \\\n",
    "                            'train','background','experience','schedule','howlong', \\\n",
    "                            'deadlift','candj','snatch','backsq','experience',\\\n",
    "                            'background','schedule','howlong'])\n",
    "data = data.drop(columns=['affiliate','team','name','athlete_id','fran','helen','grace',\\\n",
    "                            'filthy50','fgonebad','run400','run5k','pullups','train'])\n",
    "\n",
    "# Remove Outliers\n",
    "\n",
    "data = data[data['weight'] < 1500]\n",
    "data = data[data['gender'] != '--']\n",
    "data = data[data['age'] >= 18]\n",
    "data = data[(data['height'] < 96) & (data['height'] > 48)]\n",
    "\n",
    "data = data[(data['deadlift'] > 0) & (data['deadlift'] <= 1105)|((data['gender'] == 'Female') \\\n",
    "                & (data['deadlift'] <= 636))]\n",
    "data = data[(data['candj'] > 0) & (data['candj'] <= 395)]\n",
    "data = data[(data['snatch'] > 0) & (data['snatch'] <= 496)]\n",
    "data = data[(data['backsq'] > 0) & (data['backsq'] <= 1069)]\n",
    "\n",
    "# Clean Survey Data\n",
    "\n",
    "decline_dict = {'Decline to answer|': np.nan}\n",
    "data = data.replace(decline_dict)\n",
    "data = data.dropna(subset=['background','experience','schedule','howlong','eat'])\n",
    "\n",
    "data.to_csv('/Users/jackploshnick/Documents/UChicago_MADS/general_datasets/athletes_v2.csv', index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Path: \u001b[93mathletes.csv\u001b[0m\n",
      "Modified Time: 2025-10-15 19:56:52 -0400 EDT\n",
      "Size: 10774960 bytes\n",
      "Human Size: 10.8 MB\n",
      "Physical Address: local:///Users/jackploshnick/lakefs/data/block/example/data/g5fd5iiclaanklhe8940/d3o39h2claanklhe89ag\n",
      "Checksum: a813ac219d50c5af4de214f4609e5f69\n",
      "Content-Type: application/octet-stream\n"
     ]
    }
   ],
   "source": [
    "! lakectl fs upload -s /Users/jackploshnick/Documents/UChicago_MADS/general_datasets/athletes_v2.csv lakefs://athletes/assignment_1/athletes.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Branch: lakefs://athletes/assignment_1\n",
      "Commit for branch \"assignment_1\" completed.\n",
      "\n",
      "ID: \u001b[93m0a80c4bb8670a5e16b1aee95add09e3eba54a9f5235bd118cd95235e3ce636c9\u001b[0m\n",
      "Message: Add version_2 dataset\n",
      "Timestamp: 2025-10-15 19:56:53 -0400 EDT\n",
      "Parents: 5d983aea16e5b39e4852e5f700083217eb4f18c6557ac1f18be7101ee71ce849\n",
      "\n"
     ]
    }
   ],
   "source": [
    "! lakectl commit lakefs://athletes/assignment_1 -m \"Add version_2 dataset\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get version 1, do eda, run model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import subprocess\n",
    "from io import StringIO\n",
    "\n",
    "# Replace <commit_id> with your actual first commit ID\n",
    "commit_id = \"7f295082753e2062f2aa34fe22252cd8addc1a3be5037c2454ca84ca7ce335d7\"\n",
    "\n",
    "# Cat the file from that commit\n",
    "result = subprocess.run(\n",
    "    ['lakectl', 'fs', 'cat', f'lakefs://athletes/{commit_id}/athletes.csv'],\n",
    "    capture_output=True,\n",
    "    text=True\n",
    ")\n",
    "\n",
    "# Read directly into pandas\n",
    "df = pd.read_csv(StringIO(result.stdout))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R²: 0.970\n",
      "RMSE: 73.165\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "import numpy as np\n",
    "\n",
    "# Features and outcome\n",
    "features = ['age', 'height', 'weight', 'fran', 'helen', 'grace', 'filthy50', 'fgonebad', 'candj', 'run400', 'run5k', 'candj', 'snatch', 'pullups' ,'backsq']\n",
    "outcome = ['deadlift']\n",
    "\n",
    "df = df.dropna(subset=features + outcome)\n",
    "\n",
    "# Split into X and y\n",
    "X = df[features]\n",
    "y = df[outcome]\n",
    "\n",
    "# Train/test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train linear regression\n",
    "model = LinearRegression()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Predict\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Evaluate\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "\n",
    "print(f\"R²: {r2:.3f}\")\n",
    "print(f\"RMSE: {rmse:.3f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get version 2, do eda, run model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import subprocess\n",
    "from io import StringIO\n",
    "\n",
    "# Replace <commit_id> with your actual first commit ID\n",
    "commit_id = \"7f295082753e2062f2aa34fe22252cd8addc1a3be5037c2454ca84ca7ce335d7\"\n",
    "\n",
    "# Cat the file from that commit\n",
    "result = subprocess.run(\n",
    "    ['lakectl', 'fs', 'cat', f'lakefs://athletes/{commit_id}/athletes.csv'],\n",
    "    capture_output=True,\n",
    "    text=True\n",
    ")\n",
    "\n",
    "# Read directly into pandas\n",
    "df = pd.read_csv(StringIO(result.stdout))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R²: 1.000\n",
      "RMSE: 200.139\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "import numpy as np\n",
    "\n",
    "# Features and outcome\n",
    "features = ['age', 'height', 'weight', 'candj', 'snatch', 'backsq']\n",
    "outcome = ['deadlift']\n",
    "\n",
    "df = df.dropna(subset=features + outcome)\n",
    "\n",
    "# Split into X and y\n",
    "X = df[features]\n",
    "y = df[outcome]\n",
    "\n",
    "# Train/test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train linear regression\n",
    "model = LinearRegression()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Predict\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Evaluate\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "\n",
    "print(f\"R²: {r2:.3f}\")\n",
    "print(f\"RMSE: {rmse:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "V2 model is only marginally worse. .95 r2 vs .97 r2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use tensor flow privacy library with the dataset v2 and calculate the metrics for the new DP model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m463/463\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 785us/step\n",
      "R²: -0.000\n",
      "RMSE: 68936.267\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow_privacy.privacy.optimizers.dp_optimizer_keras import DPKerasSGDOptimizer\n",
    "from tensorflow_privacy.privacy.analysis import compute_dp_sgd_privacy\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "\n",
    "# -----------------------------\n",
    "# Prepare data\n",
    "# -----------------------------\n",
    "features = ['age', 'height', 'weight', 'candj', 'snatch', 'backsq']\n",
    "outcome = ['deadlift']\n",
    "\n",
    "df = df.dropna(subset=features + outcome)\n",
    "\n",
    "X = df[features].astype(np.float32)\n",
    "y = df[outcome].astype(np.float32).values.reshape(-1, 1)\n",
    "\n",
    "# Train/test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Standardize features\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# -----------------------------\n",
    "# DP Linear Regression Model\n",
    "# -----------------------------\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Input(shape=(X_train.shape[1],)),\n",
    "    tf.keras.layers.Dense(1)\n",
    "])\n",
    "\n",
    "# DP optimizer parameters\n",
    "learning_rate = 0.01\n",
    "noise_multiplier = 0.5   # adjust for privacy-accuracy tradeoff\n",
    "l2_norm_clip = 5.0\n",
    "batch_size = 32\n",
    "epochs = 100\n",
    "\n",
    "optimizer = DPKerasSGDOptimizer(\n",
    "    l2_norm_clip=l2_norm_clip,\n",
    "    noise_multiplier=noise_multiplier,\n",
    "    num_microbatches=batch_size,  # should match batch size\n",
    "    learning_rate=learning_rate\n",
    ")\n",
    "\n",
    "# Loss function (per-example)\n",
    "loss_fn = tf.keras.losses.MeanSquaredError(reduction=tf.keras.losses.Reduction.NONE)\n",
    "\n",
    "# -----------------------------\n",
    "# Custom DP training loop\n",
    "# -----------------------------\n",
    "dataset = tf.data.Dataset.from_tensor_slices((X_train, y_train))\n",
    "dataset = dataset.shuffle(buffer_size=len(X_train)).batch(batch_size)\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    for x_batch, y_batch in dataset:\n",
    "        with tf.GradientTape() as tape:\n",
    "            predictions = model(x_batch, training=True)\n",
    "            per_example_loss = loss_fn(y_batch, predictions)\n",
    "            loss = tf.reduce_mean(per_example_loss)\n",
    "\n",
    "        # Compute DP gradients (do NOT pass gradient_clip_norm)\n",
    "        grads_and_vars = optimizer._compute_gradients(\n",
    "            loss,\n",
    "            var_list=model.trainable_variables,\n",
    "            tape=tape\n",
    "        )\n",
    "        optimizer.apply_gradients(grads_and_vars)\n",
    "\n",
    "    if (epoch + 1) % 10 == 0:\n",
    "        print(f\"Epoch {epoch+1}/{epochs} completed\")\n",
    "\n",
    "# -----------------------------\n",
    "# Evaluate on test set\n",
    "# -----------------------------\n",
    "y_pred = model(X_test).numpy()\n",
    "\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "\n",
    "print(f\"DP Linear Regression R²: {r2:.3f}\")\n",
    "print(f\"DP Linear Regression RMSE: {rmse:.3f}\")\n",
    "\n",
    "# -----------------------------\n",
    "# Compute DP epsilon\n",
    "# -----------------------------\n",
    "N = X_train.shape[0]\n",
    "delta = 1e-5  # typical choice\n",
    "\n",
    "epsilon, _ = compute_dp_sgd_privacy.compute_dp_sgd_privacy(\n",
    "    n=N,\n",
    "    batch_size=batch_size,\n",
    "    noise_multiplier=noise_multiplier,\n",
    "    epochs=epochs,\n",
    "    delta=delta\n",
    ")\n",
    "\n",
    "\n",
    "# Evaluate\n",
    "r2 = r2_score(y_test, y_pred_dp)\n",
    "rmse = np.sqrt(mean_squared_error(y_test, y_pred_dp))\n",
    "\n",
    "print(f\"R²: {r2:.3f}\")\n",
    "print(f\"RMSE: {rmse:.3f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the DP approach, the accuracy of the model has degraged to the point where it is not at all useful"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
